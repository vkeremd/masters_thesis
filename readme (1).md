# Voices of Outrage: An Event Study on the Shift in Online Discourse following Notable Police Violence Deaths in the United States using a Novel Transformer-Based BERTweet Fine-Tuned Model Approach

This repository houses the code for my Masterâ€™s thesis.  The project builds a complete machineâ€‘learning pipeline that **classifies xx million tweets for racialâ€‘justiceâ€“related content (â€œperceived injusticeâ€ and "systemic racism") and maps results to U.S. counties** to be used in an event study setting.


The pipeline spans **data labelling (GPTâ€‘4), fineâ€‘tuning a BERTweet model, largeâ€‘scale inference, geolocation, and aggregation**.  Each step is modular so you can reproduce or extend individual stages.

---

## Table of Contents

- [Overview](#overview)
- [Repository Structure](#repository-structure)
- [SetupÂ &Â Dependencies](#setup--dependencies)
- [Data](#data)
- [Running the Pipeline](#running-the-pipeline)
  - [PhaseÂ 1Â â€“ Trainingâ€‘Data Creation](#phase-1--training-data-creation)
  - [PhaseÂ 2Â â€“ Model Training](#phase-2--model-training)
  - [PhaseÂ 3Â â€“ Full Classification](#phase-3--full-classification)
  - [PhaseÂ 4Â â€“ LocationÂ + County Mapping](#phase-4--location--county-mapping)
  - [PhaseÂ 5Â â€“ AnalysisÂ & Validation](#phase-5--analysis--validation)
- [Reproducibility](#reproducibility)
- [Contact](#contact)

---

## Overview

The project answers a simpleâ€”but dataâ€‘intensiveâ€”question: **How often do tweets mention racial injustice and where are those conversations happening?**

1. **GPTâ€‘4â€‘powered labelling** creates a highâ€‘quality training set.
2. **BERTweet fineâ€‘tuning** yields a domainâ€‘specific classifier.
3. **Batch inference** scores all 11â€¯M tweets for injustice.
4. **Geolocation** extracts coordinates & FIPS codes, producing countyâ€‘level tallies.
5. **Validation & exploratory analysis** confirm robustness and surface insights.

---

## Repository Structure

```text
/README.md                  # â† You are here ðŸ“
/.gitignore
/code
  â”œâ”€â”€ phase_1/               # Trainingâ€‘data creation & unit tests
  â”‚   â”œâ”€â”€ test_criteria.py
  â”‚   â”œâ”€â”€ create_training_data.py
  â”‚   â””â”€â”€ test_training_pipeline.py
  â”œâ”€â”€ phase_2/               # Model fineâ€‘tuning
  â”‚   â””â”€â”€ train_model.py
  â”œâ”€â”€ phase_3/               # Fullâ€‘dataset inference
  â”‚   â””â”€â”€ classify_all_tweets.py
  â”œâ”€â”€ phase_4/               # Geolocation & county aggregation
  â”‚   â””â”€â”€ add_location_data_fixed.py
  â”œâ”€â”€ phase_5/               # Analysis & sanity checks
  â”‚   â”œâ”€â”€ analyze_existing_csv.py
  â”‚   â”œâ”€â”€ analyze_tweet_dates.py
  â”‚   â”œâ”€â”€ check_models.py
  â”‚   â””â”€â”€ explore_csv.py
  â”œâ”€â”€ phase_6/               # Alternative / legacy scripts (kept for posterity)
  â”‚   â”œâ”€â”€ add_location_data.py
  â”‚   â”œâ”€â”€ complete_location_and_county_analysis.py
  â”‚   â”œâ”€â”€ complete_location_and_county_analysis_fixed.py
  â”‚   â””â”€â”€ complete_location_and_county_analysis_final.py
  â””â”€â”€ phase_7/               # Supporting utilities
      â”œâ”€â”€ create_county_analysis.py
      â”œâ”€â”€ quick_county_analysis.py
      â”œâ”€â”€ modify_blm_criteria.py
      â””â”€â”€ pilot_tweet.py
/data                        # Large CSVs (GitÂ LFS or ignored)
  â”œâ”€â”€ training_data_20250623_151949.csv
  â”œâ”€â”€ all_tweets_classified_20250627_120357.csv
  â”œâ”€â”€ tweets_with_locations_fixed.csv
  â””â”€â”€ blm_tweets_by_county.csv  â˜… final deliverable
```

> **Note**: The `data/` directory is **not** stored in regular Git history to keep the repo small.  Either use **GitÂ LFS** or download the files separately (see *Data* section).

---

## SetupÂ &Â Dependencies

| Requirement                 | Version              |
| --------------------------- | -------------------- |
| Python                      | â‰¥Â 3.9                |
| PyTorch                     | â‰¥Â 2.0                |
| transformers                | â‰¥Â 4.40               |
| pandas, numpy, scikitâ€‘learn | latest               |
| geopy, us, shapely          | â€”                    |
| openai                      | â‰¥Â 1.0 (PhaseÂ 1 only) |

```bash
# 1) Create & activate a virtual environment (recommended)
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2) Install core requirements
pip install -r requirements.txt  # file provided
```

If you plan to **reâ€‘label tweets** youâ€™ll also need an **OpenAI API key** exported as `OPENAI_API_KEY`.

---

## Data

1. **Raw tweets** (`tweets-*.jsonl`) are proprietary â†’ not included.
2. **Preâ€‘labelled & processed CSVs** (see `/data` above) are >1â€¯GB.  They can be downloaded from the projectâ€™s OSF page [linkâ€‘toâ€‘beâ€‘added] or recreated by running PhaseÂ 1â€“4.
3. County shapefiles come from the U.S. Census Bureau (freely available).

---

## Running the Pipeline

Below is the *happyâ€‘path* workflow.  Each phase outputs artefacts consumed by the next stage.

### PhaseÂ 1Â â€“ Trainingâ€‘Data Creation

*Create **`training_data_YYYYMMDD_HHMMSS.csv`*

```bash
cd code/phase_1
# (Optional) sanityâ€‘check criteria
python test_criteria.py
# Generate labels with GPTâ€‘4 (costs $$)
python create_training_data.py \
  --input ../../raw/tweets-0525-0615-*.jsonl \
  --out ../../data/training_data_$(date +%Y%m%d_%H%M%S).csv
```

### PhaseÂ 2Â â€“ Model Training

*Outputs **`bertweet_injustice_final/`*

```bash
cd ../phase_2
python train_model.py \
  --train_csv ../../data/training_data_20250623_151949.csv \
  --out_dir ../../models/bertweet_injustice_final
```

### PhaseÂ 3Â â€“ Full Classification

*Creates **`all_tweets_classified_YYYYMMDD_HHMMSS.csv`*

```bash
cd ../phase_3
python classify_all_tweets.py \
  --tweets ../../raw/*.jsonl \
  --model ../../models/bertweet_injustice_final \
  --out ../../data/all_tweets_classified_$(date +%Y%m%d_%H%M%S).csv
```

### PhaseÂ 4Â â€“ LocationÂ + County Mapping

*Outputs **`tweets_with_locations_fixed.csv`** & **`blm_tweets_by_county.csv`*

```bash
cd ../phase_4
python add_location_data_fixed.py \
  --classified ../../data/all_tweets_classified_20250627_120357.csv \
  --out_dir ../../data
```

### PhaseÂ 5Â â€“ AnalysisÂ & Validation (optional)

```bash
cd ../phase_5
python analyze_existing_csv.py --file ../../data/blm_tweets_by_county.csv
```

Feel free to run any script in isolationâ€”each file has *docstrings* explaining arguments & expected outputs.

---

## Reproducibility

*Fix your random seeds*Â â€“ each script sets `torch.manual_seed(42)` but feel free to override via CLI flags.\
If you deviate (e.g., different BERTweet checkpoint), update the experiment log in `experiments/`.

---

## Contact

| Name           | Email                                                    |
| -------------- | -------------------------------------------------------- |
| YourÂ NameÂ Here | [your.email@example.com](mailto\:your.email@example.com) |

Feedback, issues, and pull requests are welcome!

